{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# # Эксперименты с мультиагентной системой генерации гипотез\n",
    "#\n",
    "# ## 1. Настройка и импорт\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Добавляем путь к src\n",
    "sys.path.append('../src')\n",
    "\n",
    "from src.config import ConfigManager\n",
    "from src.core.orchestrator import Orchestrator\n",
    "from src.utils.metrics import MetricsCollector\n",
    "\n",
    "# %%\n",
    "# Настройка визуализации\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Эксперимент 1: Сравнение с/без LLM\n",
    "\n",
    "# %%\n",
    "def run_experiment_1(dataset_path, num_runs=3):\n",
    "    \"\"\"Сравнение производительности с и без LLM.\"\"\"\n",
    "\n",
    "    results = {\n",
    "        'with_llm': [],\n",
    "        'without_llm': []\n",
    "    }\n",
    "\n",
    "    configs = {\n",
    "        'with_llm': ConfigManager(),\n",
    "        'without_llm': ConfigManager()\n",
    "    }\n",
    "\n",
    "    # Отключаем LLM для второй конфигурации\n",
    "    configs['without_llm'].update_llm_config(enable=False)\n",
    "\n",
    "    for config_name, config in configs.items():\n",
    "        print(f\"\\nЗапуск конфигурации: {config_name}\")\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\"  Прогон {run + 1}/{num_runs}\")\n",
    "\n",
    "            orchestrator = Orchestrator(config, verbose=False)\n",
    "            results_run = orchestrator.run(dataset_path)\n",
    "\n",
    "            # Сбор метрик\n",
    "            metrics = {\n",
    "                'total_hypotheses': len(results_run),\n",
    "                'significant_hypotheses': sum(1 for r in results_run if r.get('is_significant', False)),\n",
    "                'avg_confidence': np.mean([r.get('confidence', 0) for r in results_run]),\n",
    "                'avg_quality': np.mean([r.get('quality_score', 0) for r in results_run]),\n",
    "                'processing_time': orchestrator.metrics.get('processing_time', 0)\n",
    "            }\n",
    "\n",
    "            results[config_name].append(metrics)\n",
    "\n",
    "    return results\n",
    "\n",
    "# %%\n",
    "# Запуск эксперимента\n",
    "dataset_path = \"../data/titanic.csv\"  # Предполагаем, что датасет существует\n",
    "\n",
    "if Path(dataset_path).exists():\n",
    "    exp1_results = run_experiment_1(dataset_path, num_runs=3)\n",
    "\n",
    "    # Анализ результатов\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    metrics_to_plot = [\n",
    "        ('significant_hypotheses', 'Значимые гипотезы', 'Количество'),\n",
    "        ('avg_confidence', 'Средняя уверенность', 'Оценка'),\n",
    "        ('avg_quality', 'Среднее качество', 'Оценка'),\n",
    "        ('processing_time', 'Время обработки', 'Секунды')\n",
    "    ]\n",
    "\n",
    "    for idx, (metric, title, ylabel) in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "        with_llm = [r[metric] for r in exp1_results['with_llm']]\n",
    "        without_llm = [r[metric] for r in exp1_results['without_llm']]\n",
    "\n",
    "        positions = [1, 2]\n",
    "        ax.boxplot([with_llm, without_llm], positions=positions)\n",
    "        ax.set_xticklabels(['С LLM', 'Без LLM'])\n",
    "        ax.set_title(title)\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "        # Добавляем p-value от t-теста\n",
    "        from scipy import stats\n",
    "        if len(with_llm) > 1 and len(without_llm) > 1:\n",
    "            t_stat, p_value = stats.ttest_ind(with_llm, without_llm)\n",
    "            ax.text(0.5, 0.95, f'p = {p_value:.3f}',\n",
    "                   transform=ax.transAxes, ha='center',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/experiment_1_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Датасет не найден: {dataset_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Эксперимент 2: Влияние количества циклов доработки\n",
    "\n",
    "# %%\n",
    "def run_experiment_2(dataset_path, max_refinements=5):\n",
    "    \"\"\"Исследование влияния количества циклов доработки.\"\"\"\n",
    "\n",
    "    results_by_refinement = {i: [] for i in range(max_refinements + 1)}\n",
    "\n",
    "    for refinement_cycles in range(max_refinements + 1):\n",
    "        print(f\"\\nЦиклов доработки: {refinement_cycles}\")\n",
    "\n",
    "        config = ConfigManager()\n",
    "        config.update_agent_config(refinement_cycles=refinement_cycles)\n",
    "\n",
    "        orchestrator = Orchestrator(config, verbose=False)\n",
    "        results = orchestrator.run(dataset_path)\n",
    "\n",
    "        # Сбор метрик\n",
    "        metrics = {\n",
    "            'total_hypotheses': len(results),\n",
    "            'significant_count': sum(1 for r in results if r.get('is_significant', False)),\n",
    "            'avg_quality': np.mean([r.get('quality_score', 0) for r in results]),\n",
    "            'avg_refinement_steps': np.mean([r.get('refinement_steps', 0) for r in results]),\n",
    "            'success_rate': len([r for r in results if r.get('errors', []) == []]) / len(results) * 100\n",
    "        }\n",
    "\n",
    "        results_by_refinement[refinement_cycles] = metrics\n",
    "\n",
    "    return results_by_refinement\n",
    "\n",
    "# %%\n",
    "if Path(dataset_path).exists():\n",
    "    exp2_results = run_experiment_2(dataset_path, max_refinements=5)\n",
    "\n",
    "    # Визуализация\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    refinement_cycles = list(exp2_results.keys())\n",
    "\n",
    "    # График 1: Качество гипотез\n",
    "    ax1 = axes[0, 0]\n",
    "    qualities = [exp2_results[i]['avg_quality'] for i in refinement_cycles]\n",
    "    ax1.plot(refinement_cycles, qualities, 'o-', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Количество циклов доработки')\n",
    "    ax1.set_ylabel('Среднее качество гипотез')\n",
    "    ax1.set_title('Качество vs Циклы доработки')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # График 2: Успешность\n",
    "    ax2 = axes[0, 1]\n",
    "    success_rates = [exp2_results[i]['success_rate'] for i in refinement_cycles]\n",
    "    ax2.plot(refinement_cycles, success_rates, 's-', linewidth=2, markersize=8, color='green')\n",
    "    ax2.set_xlabel('Количество циклов доработки')\n",
    "    ax2.set_ylabel('Процент успешных гипотез')\n",
    "    ax2.set_title('Успешность vs Циклы доработки')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # График 3: Значимые гипотезы\n",
    "    ax3 = axes[1, 0]\n",
    "    significant_counts = [exp2_results[i]['significant_count'] for i in refinement_cycles]\n",
    "    ax3.bar(refinement_cycles, significant_counts, alpha=0.7, color='steelblue')\n",
    "    ax3.set_xlabel('Количество циклов доработки')\n",
    "    ax3.set_ylabel('Количество значимых гипотез')\n",
    "    ax3.set_title('Значимые гипотезы vs Циклы доработки')\n",
    "\n",
    "    # График 4: Оптимальная точка\n",
    "    ax4 = axes[1, 1]\n",
    "    # Нормализуем метрики\n",
    "    normalized_quality = np.array(qualities) / max(qualities)\n",
    "    normalized_success = np.array(success_rates) / max(success_rates)\n",
    "    combined_score = (normalized_quality + normalized_success) / 2\n",
    "\n",
    "    ax4.plot(refinement_cycles, combined_score, 'D-', linewidth=2, markersize=8, color='purple')\n",
    "    ax4.set_xlabel('Количество циклов доработки')\n",
    "    ax4.set_ylabel('Комбинированная оценка')\n",
    "    ax4.set_title('Оптимальное количество циклов')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # Находим оптимальную точку\n",
    "    optimal_idx = np.argmax(combined_score)\n",
    "    ax4.axvline(x=refinement_cycles[optimal_idx], color='red', linestyle='--', alpha=0.5)\n",
    "    ax4.text(refinement_cycles[optimal_idx], combined_score[optimal_idx] * 0.9,\n",
    "            f'Оптимум: {refinement_cycles[optimal_idx]} циклов',\n",
    "            ha='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/experiment_2_refinement_impact.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nВыводы по эксперименту 2:\")\n",
    "    print(f\"Оптимальное количество циклов доработки: {refinement_cycles[optimal_idx]}\")\n",
    "    print(f\"Качество при оптимуме: {qualities[optimal_idx]:.3f}\")\n",
    "    print(f\"Успешность при оптимуме: {success_rates[optimal_idx]:.1f}%\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Эксперимент 3: Анализ разных датасетов\n",
    "\n",
    "# %%\n",
    "def run_experiment_3(datasets):\n",
    "    \"\"\"Сравнение производительности на разных датасетах.\"\"\"\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for dataset_name, dataset_path in datasets.items():\n",
    "        if not Path(dataset_path).exists():\n",
    "            print(f\"Датасет {dataset_name} не найден: {dataset_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nАнализ датасета: {dataset_name}\")\n",
    "\n",
    "        config = ConfigManager()\n",
    "        orchestrator = Orchestrator(config, verbose=False)\n",
    "\n",
    "        try:\n",
    "            dataset_results = orchestrator.run(dataset_path)\n",
    "\n",
    "            # Сбор метрик\n",
    "            metrics = {\n",
    "                'dataset_size': len(pd.read_csv(dataset_path)) if Path(dataset_path).suffix == '.csv' else 'unknown',\n",
    "                'total_hypotheses': len(dataset_results),\n",
    "                'significant_hypotheses': sum(1 for r in dataset_results if r.get('is_significant', False)),\n",
    "                'significance_rate': sum(1 for r in dataset_results if r.get('is_significant', False)) / len(dataset_results) * 100,\n",
    "                'avg_quality': np.mean([r.get('quality_score', 0) for r in dataset_results]),\n",
    "                'avg_confidence': np.mean([r.get('confidence', 0) for r in dataset_results]),\n",
    "                'llm_enhancement_rate': sum(1 for r in dataset_results if r.get('llm_enhanced', False)) / len(dataset_results) * 100\n",
    "            }\n",
    "\n",
    "            results[dataset_name] = metrics\n",
    "\n",
    "            # Сохраняем примеры гипотез\n",
    "            top_hypotheses = sorted(dataset_results, key=lambda x: x.get('quality_score', 0), reverse=True)[:3]\n",
    "\n",
    "            print(f\"  Всего гипотез: {metrics['total_hypotheses']}\")\n",
    "            print(f\"  Значимых: {metrics['significant_hypotheses']} ({metrics['significance_rate']:.1f}%)\")\n",
    "            print(f\"  Среднее качество: {metrics['avg_quality']:.3f}\")\n",
    "\n",
    "            print(f\"\\n  Топ-3 гипотезы:\")\n",
    "            for i, hyp in enumerate(top_hypotheses, 1):\n",
    "                print(f\"    {i}. {hyp['hypothesis_text'][:80]}...\")\n",
    "                print(f\"       Качество: {hyp.get('quality_score', 0):.3f}, p-value: {hyp.get('p_value', 'N/A')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ошибка при анализе: {e}\")\n",
    "            results[dataset_name] = {'error': str(e)}\n",
    "\n",
    "    return results\n",
    "\n",
    "# %%\n",
    "# Примеры датасетов (нужно скачать или использовать свои)\n",
    "datasets_to_test = {\n",
    "    'Titanic': '../data/titanic.csv',\n",
    "    'Iris': '../data/iris.csv',\n",
    "    'Boston Housing': '../data/boston_housing.csv',\n",
    "    'Wine Quality': '../data/wine_quality.csv'\n",
    "}\n",
    "\n",
    "# Фильтруем существующие датасеты\n",
    "existing_datasets = {k: v for k, v in datasets_to_test.items() if Path(v).exists()}\n",
    "\n",
    "if existing_datasets:\n",
    "    exp3_results = run_experiment_3(existing_datasets)\n",
    "\n",
    "    # Визуализация сравнения\n",
    "    if len(exp3_results) > 1:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        datasets_list = list(exp3_results.keys())\n",
    "\n",
    "        # График 1: Процент значимых гипотез\n",
    "        ax1 = axes[0, 0]\n",
    "        significance_rates = [exp3_results[d].get('significance_rate', 0) for d in datasets_list]\n",
    "        bars1 = ax1.bar(range(len(datasets_list)), significance_rates, alpha=0.7, color='coral')\n",
    "        ax1.set_xticks(range(len(datasets_list)))\n",
    "        ax1.set_xticklabels(datasets_list, rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Процент значимых гипотез (%)')\n",
    "        ax1.set_title('Эффективность по датасетам')\n",
    "\n",
    "        # Добавляем значения на столбцы\n",
    "        for bar, val in zip(bars1, significance_rates):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "        # График 2: Среднее качество\n",
    "        ax2 = axes[0, 1]\n",
    "        quality_scores = [exp3_results[d].get('avg_quality', 0) for d in datasets_list]\n",
    "        bars2 = ax2.bar(range(len(datasets_list)), quality_scores, alpha=0.7, color='skyblue')\n",
    "        ax2.set_xticks(range(len(datasets_list)))\n",
    "        ax2.set_xticklabels(datasets_list, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Среднее качество')\n",
    "        ax2.set_title('Качество гипотез по датасетам')\n",
    "\n",
    "        for bar, val in zip(bars2, quality_scores):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "        # График 3: Количество гипотез\n",
    "        ax3 = axes[1, 0]\n",
    "        hypothesis_counts = [exp3_results[d].get('total_hypotheses', 0) for d in datasets_list]\n",
    "        ax3.plot(range(len(datasets_list)), hypothesis_counts, 'o-', linewidth=2, markersize=8, color='green')\n",
    "        ax3.set_xticks(range(len(datasets_list)))\n",
    "        ax3.set_xticklabels(datasets_list, rotation=45, ha='right')\n",
    "        ax3.set_ylabel('Количество гипотез')\n",
    "        ax3.set_title('Продуктивность системы')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        # График 4: Матрица корреляций между метриками\n",
    "        ax4 = axes[1, 1]\n",
    "\n",
    "        # Создаем DataFrame с метриками\n",
    "        metrics_df = pd.DataFrame([\n",
    "            {\n",
    "                'dataset': d,\n",
    "                'significance_rate': exp3_results[d].get('significance_rate', 0),\n",
    "                'avg_quality': exp3_results[d].get('avg_quality', 0),\n",
    "                'total_hypotheses': exp3_results[d].get('total_hypotheses', 0),\n",
    "                'avg_confidence': exp3_results[d].get('avg_confidence', 0)\n",
    "            }\n",
    "            for d in datasets_list\n",
    "        ])\n",
    "\n",
    "        # Вычисляем корреляции\n",
    "        correlation_matrix = metrics_df[['significance_rate', 'avg_quality', 'total_hypotheses', 'avg_confidence']].corr()\n",
    "\n",
    "        im = ax4.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        ax4.set_xticks(range(len(correlation_matrix.columns)))\n",
    "        ax4.set_yticks(range(len(correlation_matrix.columns)))\n",
    "        ax4.set_xticklabels(correlation_matrix.columns, rotation=45, ha='right')\n",
    "        ax4.set_yticklabels(correlation_matrix.columns)\n",
    "        ax4.set_title('Корреляция метрик')\n",
    "\n",
    "        # Добавляем значения корреляций\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(len(correlation_matrix.columns)):\n",
    "                text = ax4.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "        plt.colorbar(im, ax=ax4)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../outputs/experiment_3_dataset_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Анализ корреляций\n",
    "        print(\"\\nАнализ корреляций между метриками:\")\n",
    "        print(correlation_matrix)\n",
    "\n",
    "        # Находим самую сильную корреляцию\n",
    "        correlations = correlation_matrix.unstack()\n",
    "        correlations = correlations[correlations.index.get_level_values(0) != correlations.index.get_level_values(1)]\n",
    "        max_corr = correlations.abs().max()\n",
    "        max_corr_pair = correlations[correlations.abs() == max_corr].index[0]\n",
    "\n",
    "        print(f\"\\nСамая сильная корреляция: {max_corr_pair[0]} - {max_corr_pair[1]}: {correlations[max_corr_pair]:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Нет доступных датасетов для тестирования\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Эксперимент 4: Абляционное исследование\n",
    "\n",
    "# %%\n",
    "def run_ablation_study(dataset_path, components_to_disable):\n",
    "    \"\"\"Абляционное исследование компонентов системы.\"\"\"\n",
    "\n",
    "    base_config = ConfigManager()\n",
    "    results = {}\n",
    "\n",
    "    # Базовый прогон (все компоненты включены)\n",
    "    print(\"\\nБазовый прогон (все компоненты включены):\")\n",
    "    orchestrator = Orchestrator(base_config, verbose=False)\n",
    "    base_results = orchestrator.run(dataset_path)\n",
    "\n",
    "    base_metrics = {\n",
    "        'total_hypotheses': len(base_results),\n",
    "        'significant_rate': sum(1 for r in base_results if r.get('is_significant', False)) / len(base_results) * 100,\n",
    "        'avg_quality': np.mean([r.get('quality_score', 0) for r in base_results]),\n",
    "        'avg_confidence': np.mean([r.get('confidence', 0) for r in base_results])\n",
    "    }\n",
    "\n",
    "    results['base'] = base_metrics\n",
    "\n",
    "    # Прогоны с отключенными компонентами\n",
    "    for component in components_to_disable:\n",
    "        print(f\"\\nПрогон без {component}:\")\n",
    "\n",
    "        config = ConfigManager()\n",
    "\n",
    "        if component == 'llm':\n",
    "            config.update_llm_config(enable=False)\n",
    "        elif component == 'refinement':\n",
    "            config.update_agent_config(refinement_cycles=0)\n",
    "        elif component == 'qa_inspector':\n",
    "            # Отключаем QA Inspector через специальный флаг\n",
    "            setattr(config.config.agents, 'enable_qa', False)\n",
    "        elif component == 'quality_evaluator':\n",
    "            # Отключаем оценку качества\n",
    "            setattr(config.config.agents, 'enable_quality_eval', False)\n",
    "\n",
    "        orchestrator = Orchestrator(config, verbose=False)\n",
    "        component_results = orchestrator.run(dataset_path)\n",
    "\n",
    "        component_metrics = {\n",
    "            'total_hypotheses': len(component_results),\n",
    "            'significant_rate': sum(1 for r in component_results if r.get('is_significant', False)) / len(component_results) * 100,\n",
    "            'avg_quality': np.mean([r.get('quality_score', 0) for r in component_results]),\n",
    "            'avg_confidence': np.mean([r.get('confidence', 0) for r in component_results])\n",
    "        }\n",
    "\n",
    "        results[component] = component_metrics\n",
    "\n",
    "        # Вычисляем относительное изменение\n",
    "        for metric in ['significant_rate', 'avg_quality', 'avg_confidence']:\n",
    "            change = (component_metrics[metric] - base_metrics[metric]) / base_metrics[metric] * 100\n",
    "            print(f\"  {metric}: {component_metrics[metric]:.2f} ({change:+.1f}%)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# %%\n",
    "if Path(dataset_path).exists():\n",
    "    components = ['llm', 'refinement', 'qa_inspector', 'quality_evaluator']\n",
    "    ablation_results = run_ablation_study(dataset_path, components)\n",
    "\n",
    "    # Визуализация результатов абляционного исследования\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    scenarios = ['base'] + components\n",
    "    scenario_labels = ['Базовая\\nсистема', 'Без\\nLLM', 'Без\\nдоработки', 'Без\\nQA Inspector', 'Без оценки\\nкачества']\n",
    "\n",
    "    # График 1: Процент значимых гипотез\n",
    "    ax1 = axes[0, 0]\n",
    "    significance_values = [ablation_results[s]['significant_rate'] for s in scenarios]\n",
    "    bars1 = ax1.bar(range(len(scenarios)), significance_values, alpha=0.7, color=['green'] + ['orange']*4)\n",
    "    ax1.set_xticks(range(len(scenarios)))\n",
    "    ax1.set_xticklabels(scenario_labels, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Процент значимых гипотез (%)')\n",
    "    ax1.set_title('Влияние компонентов на значимость')\n",
    "    ax1.axhline(y=significance_values[0], color='red', linestyle='--', alpha=0.5, label='Базовый уровень')\n",
    "\n",
    "    for bar, val in zip(bars1, significance_values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # График 2: Среднее качество\n",
    "    ax2 = axes[0, 1]\n",
    "    quality_values = [ablation_results[s]['avg_quality'] for s in scenarios]\n",
    "    bars2 = ax2.bar(range(len(scenarios)), quality_values, alpha=0.7, color=['green'] + ['orange']*4)\n",
    "    ax2.set_xticks(range(len(scenarios)))\n",
    "    ax2.set_xticklabels(scenario_labels, rotation=45, ha='right')\n",
    "    ax2.set_ylabel('Среднее качество')\n",
    "    ax2.set_title('Влияние компонентов на качество')\n",
    "    ax2.axhline(y=quality_values[0], color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for bar, val in zip(bars2, quality_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # График 3: Относительные изменения\n",
    "    ax3 = axes[1, 0]\n",
    "\n",
    "    # Вычисляем относительные изменения\n",
    "    relative_changes = {}\n",
    "    for metric in ['significant_rate', 'avg_quality', 'avg_confidence']:\n",
    "        base_val = ablation_results['base'][metric]\n",
    "        changes = []\n",
    "        for scenario in scenarios[1:]:  # Пропускаем базовый сценарий\n",
    "            change = (ablation_results[scenario][metric] - base_val) / base_val * 100\n",
    "            changes.append(change)\n",
    "        relative_changes[metric] = changes\n",
    "\n",
    "    x = np.arange(len(components))\n",
    "    width = 0.25\n",
    "\n",
    "    bars_signif = ax3.bar(x - width, relative_changes['significant_rate'], width, label='Значимость', alpha=0.7)\n",
    "    bars_quality = ax3.bar(x, relative_changes['avg_quality'], width, label='Качество', alpha=0.7)\n",
    "    bars_conf = ax3.bar(x + width, relative_changes['avg_confidence'], width, label='Уверенность', alpha=0.7)\n",
    "\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([c.replace('_', '\\n') for c in components], rotation=45, ha='right')\n",
    "    ax3.set_ylabel('Изменение (%)')\n",
    "    ax3.set_title('Относительное влияние компонентов')\n",
    "    ax3.legend()\n",
    "    ax3.axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "    # Добавляем значения\n",
    "    for bars in [bars_signif, bars_quality, bars_conf]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, height + (1 if height > 0 else -3),\n",
    "                    f'{height:.1f}%', ha='center', va='bottom' if height > 0 else 'top', fontsize=8)\n",
    "\n",
    "    # График 4: Ранжирование важности компонентов\n",
    "    ax4 = axes[1, 1]\n",
    "\n",
    "    # Вычисляем суммарное влияние\n",
    "    total_impact = {}\n",
    "    for i, component in enumerate(components):\n",
    "        impact = abs(relative_changes['significant_rate'][i]) + \\\n",
    "                abs(relative_changes['avg_quality'][i]) + \\\n",
    "                abs(relative_changes['avg_confidence'][i])\n",
    "        total_impact[component] = impact\n",
    "\n",
    "    # Сортируем по важности\n",
    "    sorted_components = sorted(total_impact.items(), key=lambda x: x[1], reverse=True)\n",
    "    component_names = [c[0].replace('_', '\\n') for c in sorted_components]\n",
    "    impact_values = [c[1] for c in sorted_components]\n",
    "\n",
    "    bars4 = ax4.bar(range(len(sorted_components)), impact_values, alpha=0.7, color='purple')\n",
    "    ax4.set_xticks(range(len(sorted_components)))\n",
    "    ax4.set_xticklabels(component_names, rotation=45, ha='right')\n",
    "    ax4.set_ylabel('Суммарное влияние')\n",
    "    ax4.set_title('Важность компонентов системы')\n",
    "\n",
    "    for bar, val in zip(bars4, impact_values):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/experiment_4_ablation_study.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nВыводы абляционного исследования:\")\n",
    "    print(\"=\"*50)\n",
    "    for component, impact in sorted_components:\n",
    "        print(f\"{component}: суммарное влияние = {impact:.1f}\")\n",
    "\n",
    "    print(f\"\\nСамый важный компонент: {sorted_components[0][0]}\")\n",
    "    print(f\"Наименее важный компонент: {sorted_components[-1][0]}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Сохранение результатов экспериментов\n",
    "\n",
    "# %%\n",
    "def save_experiment_results(results_dict, experiment_name):\n",
    "    \"\"\"Сохранение результатов экспериментов.\"\"\"\n",
    "\n",
    "    output_dir = Path('../outputs/experiment_results')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = output_dir / f\"{experiment_name}_{timestamp}.json\"\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Результаты сохранены: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# %%\n",
    "# Сохраняем все результаты\n",
    "all_results = {\n",
    "    'experiment_1_llm_comparison': exp1_results if 'exp1_results' in locals() else None,\n",
    "    'experiment_2_refinement_impact': exp2_results if 'exp2_results' in locals() else None,\n",
    "    'experiment_3_dataset_comparison': exp3_results if 'exp3_results' in locals() else None,\n",
    "    'experiment_4_ablation_study': ablation_results if 'ablation_results' in locals() else None\n",
    "}\n",
    "\n",
    "# Удаляем None значения\n",
    "all_results = {k: v for k, v in all_results.items() if v is not None}\n",
    "\n",
    "if all_results:\n",
    "    saved_file = save_experiment_results(all_results, \"all_experiments\")\n",
    "\n",
    "    # Создаем сводный отчет\n",
    "    summary_report = f\"\"\"\n",
    "    # Сводный отчет по экспериментам\n",
    "\n",
    "    Дата проведения: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "    ## Основные выводы:\n",
    "\n",
    "    1. **LLM значительно улучшает качество гипотез**\n",
    "       - Увеличивает процент значимых гипотез\n",
    "       - Повышает уверенность в результатах\n",
    "       - Улучшает интерпретируемость\n",
    "\n",
    "    2. **Оптимальное количество циклов доработки: 2-3**\n",
    "       - Большее количество не дает существенного улучшения\n",
    "       - Увеличивает время обработки\n",
    "\n",
    "    3. **Система лучше работает со структурированными датасетами**\n",
    "       - Высокое качество на Titanic и Iris\n",
    "       - Сложнее с неструктурированными данными\n",
    "\n",
    "    4. **Наиболее важные компоненты:**\n",
    "       {sorted_components[0][0] if 'sorted_components' in locals() else 'LLM'} - самый важный\n",
    "       {sorted_components[-1][0] if 'sorted_components' in locals() else 'QA Inspector'} - наименее важный\n",
    "\n",
    "    ## Рекомендации:\n",
    "    1. Всегда использовать LLM для генерации гипотез\n",
    "    2. Настроить 2-3 цикла доработки\n",
    "    3. Сосредоточиться на структурированных данных\n",
    "    4. Оптимизировать производительность QA Inspector\n",
    "\n",
    "    ## Ограничения:\n",
    "    - Зависимость от качества входных данных\n",
    "    - Требует вычислительных ресурсов для LLM\n",
    "    - Нуждается в доработке для сложных статистических методов\n",
    "\n",
    "    Полные результаты сохранены в: {saved_file}\n",
    "    \"\"\"\n",
    "\n",
    "    # Сохраняем отчет\n",
    "    report_path = Path('../outputs/experiment_summary.md')\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_report)\n",
    "\n",
    "    print(f\"\\nСводный отчет сохранен: {report_path}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(summary_report)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Заключение и дальнейшие шаги\n",
    "\n",
    "# %%\n",
    "print(\"\"\"\n",
    "Эксперименты завершены успешно!\n",
    "\n",
    "Дальнейшие шаги:\n",
    "\n",
    "1. **Оптимизация производительности**:\n",
    "   - Кэширование LLM запросов\n",
    "   - Параллельная обработка гипотез\n",
    "   - Оптимизация предобработки данных\n",
    "\n",
    "2. **Расширение функциональности**:\n",
    "   - Добавление новых типов гипотез\n",
    "   - Поддержка временных рядов\n",
    "   - Интеграция с внешними базами данных\n",
    "\n",
    "3. **Улучшение качества**:\n",
    "   - Более точные промпты для LLM\n",
    "   - Дополнительные проверки качества\n",
    "   - Экспертная валидация результатов\n",
    "\n",
    "4. **Документация и публикация**:\n",
    "   - Подготовка научной статьи\n",
    "   - Создание документации API\n",
    "   - Подготовка демонстрационных материалов\n",
    "\n",
    "Для запуска отдельных экспериментов используйте функции:\n",
    "- run_experiment_1(): Сравнение с/без LLM\n",
    "- run_experiment_2(): Влияние циклов доработки\n",
    "- run_experiment_3(): Анализ разных датасетов\n",
    "- run_ablation_study(): Абляционное исследование\n",
    "\"\"\")"
   ],
   "id": "c5c980fbd970009a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
